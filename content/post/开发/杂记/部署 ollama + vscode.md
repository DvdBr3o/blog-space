---
title: éƒ¨ç½² ollama + vscode
date: 2024-12-17T18:12:13+08:00
lastmod: 2025-03-09T13:46:59+08:00
tags:
  - ollama
  - ai
publish: true
slug: éƒ¨ç½²-ollama-and-vscode
---

>[!info] å‰æƒ…æè¦
>cursor è´µï¼Œæƒ³æƒ³ç”¨æœ¬åœ° ollama æ˜¯ä¸æ˜¯å°±ä¸ç”¨ä»˜è´¹äº†ğŸ¤¤

## æœ¬åœ°

ä¸»åŠ›æœºå¤©é€‰æ„Ÿè§‰åº”è¯¥å¸¦çš„åŠ¨å¤§æ¨¡å‹ï¼Œæ‰€ä»¥å¯ä»¥è¯•è¯•æœ¬åœ°éƒ¨ç½²ã€‚

### å®‰è£… ollama

```bash
scoop install ollama
```

### é…ç½® ollama å¼€æœºè‡ªå¯

åˆ›å»ºä¸€ä¸ªå¿«æ·æ–¹å¼ï¼Œç›®æ ‡æ˜¯ `<path/to>/ollama.exe serve`ã€‚

`Win+R`ï¼Œå°†å¿«æ·æ–¹å¼ç²˜è´´åˆ° `shell:startup` çš„è·¯å¾„ä¸‹ã€‚

### é…ç½®æ¨¡å‹

```bash
olloma pull llama3.2
# olloma pull llama3.1:8b # è¿™ä¸ªæ˜¯ continue å®˜æ–¹æ¨èä½†æ˜¯æˆ‘æ²¡ä¸‹
```

### é…ç½® continue

vscode å®‰è£… continueã€‚

åœ¨ continue çš„ config.json ä¸­æ·»åŠ ï¼š

```json
{
  "models": [
    // ...
Â  Â  {
Â  Â  Â  "title": "Llama",
Â  Â  Â  "provider": "ollama",
Â  Â  Â  "model": "llama3.2"
Â  Â  }
Â  ],
Â  // ...
}
```

ğŸ”—**æ›´å¤šæ¨¡æ€**ï¼š

```cardlink
url: https://docs.continue.dev/customize/context-providers#built-in-context-providers
title: "Context providers | Continue"
description: "Type '@' to select content to the LLM as context"
host: docs.continue.dev
favicon: https://docs.continue.dev/img/favicon.ico
image: https://docs.continue.dev/img/continue-social-card.png
```


Then you are good to go ~ ğŸ‰
## è¿œç¨‹

å˜»å˜»ä¹‹å‰ä¹°çš„æœåŠ¡å™¨æ´¾ä¸Šç”¨åœºäº†ï¼Œè¿œç¨‹éƒ¨ç½²ä¸€ä¸ªåœ¨å¤–é¢è½»è–„æœ¬ ~~(è¿˜æ²¡ä¹°ï¼Œæœ‰é’±æ˜¯é¥é¥æ— æœŸ)~~ ä¹Ÿèƒ½ç”¨ä¸Š ollamaã€‚