---
title: 交叉熵
date: 2025-07-15T08:22:44+08:00
lastmod: 2025-07-16T18:07:40+08:00
tags:
  - 深度学习
categories: 深度学习
publish: true
---

## 熵

熵(Entropy) = 最小平均编码长度 = E(编码长度)

$$
\mathrm{Entropy}=-\sum_{i=1}^{n} p(i)\cdot \log_{2}p(i)
$$

>[!hint] 推导
> 
> $$
> \begin{align}
> \mathrm{Entropy} & =E(v) \\
>  & =\sum_{i=1}^{n} p(i) \cdot v_{i} \\
>  & =\sum_{i=1}^{n} p(i)\cdot \log_{2}N \\
>  & =-\sum_{i=1}^{n} p(i)\log_{2} \frac{1}{N} \\
>  & =-\sum_{i=1}^{n} p(i)\log_{2}p(i)
> \end{align}
> $$

>[!note] 编码长度
>编码长度 $v=\log_{2}N$
>
>i.e. N种可能只需要 $\log_{2}N$ 位二进制数就可以表示所有情况

## 交叉熵

$$
\mathrm{Entropy}(p, q) = -\sum_{i=1}^{n} p(i)\cdot \log_{2}q(i)
$$
>[!note] 定性性质
>分布 $p,q$ 越相似 $\implies$ 交叉熵
>- 越 $\to$ 熵
>- 越小



