---
title: 激活函数
date: 2025-07-15T09:46:33+08:00
lastmod: 2025-07-15T10:13:34+08:00
tags:
  - 深度学习
  - 神经网络
categories: 深度学习
publish: true
---

## 概览

- sgn $$\mathrm{sgn}(x)=\left\{\begin{align} & 1, & x>0 \\  & 0, & x=0 \\  & -1, & x<0\end{align}\right. $$
- sigmoid $$\sigma(x)=\mathrm{sigmoid}(x)=\frac{1}{1+\exp(-x)}$$
	![image.png](https://s2.loli.net/2025/07/15/pGWD475gdXwRqhI.png)

- tanh $$\tanh (x)=\frac{1-\exp(-2x)}{1+\exp(-2x)}$$
	![image.png](https://s2.loli.net/2025/07/15/v8fXRq476TUeDLC.png)

- ReLU $$\mathrm{ReLU}(x)=\mathrm{max}(x,0)$$
	![image.png](https://s2.loli.net/2025/07/15/TpWVivftzB8bPEd.png)

- Leaky ReLU $$\mathrm{LeakyReLU}(x)=\left\{\begin{align} & x, & x>0\\  & \alpha x, & x\leq 0 \end{align}\right.$$ 其中 $\alpha$ 很小，$\to 0$，经验上 $\alpha=0.01$ 或者 $\alpha\sim N(0,1)$
	![image.png](https://s2.loli.net/2025/07/15/gc629WILv4U8ebw.png)

- ELU $$\mathrm{ELU}(x)=\left\{\begin{align} & x, & x>0 \\  & \alpha(e^{x}-1), & x\leq 0\end{align}\right.$$
	![image.png](https://s2.loli.net/2025/07/15/9vLVmABRqaXNosf.png)

- Swish $$\mathrm{Swish}(x)=x\cdot\sigma(\beta \cdot x)$$
	![image.png](https://s2.loli.net/2025/07/15/CVMlnTU4PDo2KOc.png)


## 对比

| 激活函数       | 优点           | 缺点                  |
| ---------- | ------------ | ------------------- |
| sigmoid    |              | 单侧分布<br>双侧饱和        |
| tanh       | 双侧分布         | 双侧饱和                |
| ReLU       | 单侧饱和         | 未双侧分布<br>Dying ReLU |
| Leaky ReLU | 双侧分布         |                     |
| ELU        | 单侧饱和<br>双侧分布 |                     |
| Swish      | 单侧饱和<br>双侧分布 |                     |

>[!note] 梯度饱和
>指激活函数某些域下梯度 $\to 0$ 的现象
>导致梯度下降慢，模型收敛效果差，甚至失活

>[!note] BP 失活
>指梯度为 $0$，[梯度下降法](./%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.md)更新权重参数时为 $0$ 的现象
>
>e.g. Dying ReLU

## ReLU

## LeakyReLU

### 随机 LeakyReLU

把 $\alpha$ 随机取值，总体上 $\alpha \sim N(0,1)$

### PReLU

把 $\alpha$ 作为需要学习的参数

## ELU

